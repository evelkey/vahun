{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import Levenshtein\n",
    "\n",
    "class Autoencoder():\n",
    "    def __init__(self,\n",
    "                 tf_session, inputdim,\n",
    "                 logger,\n",
    "                 layerlist,\n",
    "                 encode_index,\n",
    "                 corpus,\n",
    "                 optimizer = tf.train.AdamOptimizer(),\n",
    "                 nonlinear=tf.nn.sigmoid,\n",
    "                 disp_step=30,\n",
    "                charnum=0,\n",
    "                maxlen=0):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.charnum=charnum\n",
    "        self.maxlen=maxlen\n",
    "        self.corpus=corpus\n",
    "        self.logger=logger\n",
    "        \n",
    "        self.layerlist=layerlist\n",
    "        self.layernum=len(layerlist)\n",
    "        self.n_input = inputdim\n",
    "        self.encode_index=encode_index\n",
    "        self.display_step=disp_step\n",
    "        self.nonlinear=nonlinear\n",
    "          \n",
    "        self.create_graph()\n",
    "        \n",
    "        self.size=0\n",
    "        nums=[self.n_input,layerlist]\n",
    "        for i in range(1,len(nums)):\n",
    "            self.size+=4*layerlist[i]*layerlist[i-1]\n",
    "        \n",
    "\n",
    "    def create_graph(self):\n",
    "        raise NotImplementedError()\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights\n",
    "        self._create_layers()\n",
    "        # cost\n",
    "        self.cost =  0.5*tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.y), 2.0))\n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf_session\n",
    "        self.sess.run(init)\n",
    "        self.saver = tf.train.Saver()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        raise NotImplementedError()\n",
    "        all_weights = dict()\n",
    "        \n",
    "        all_weights['w1']=tf.Variable(self.xavier_init(self.n_input, self.layerlist[0]))\n",
    "        all_weights['b1'] = tf.Variable(tf.random_normal([self.layerlist[0]], dtype=tf.float32))\n",
    "        \n",
    "        for i in range(1,self.layernum):\n",
    "            all_weights['w'+str(i+1)]=tf.Variable(self.xavier_init(self.layerlist[i-1], self.layerlist[i]))\n",
    "            all_weights['b'+str(i+1)] = tf.Variable(tf.random_normal([self.layerlist[i]], dtype=tf.float32))\n",
    "\n",
    "        return all_weights\n",
    "    \n",
    "    def _create_layers(self,nonlinearity):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        layer=nonlinearity(tf.add(tf.matmul(self.x, self.weights['w1']), self.weights['b1']))\n",
    "        self.encoded=layer\n",
    "        for i in range(1,self.layernum-1):\n",
    "            if i==self.encode_index:\n",
    "                self.encoded=layer\n",
    "            layer=nonlinearity(tf.add(tf.matmul(layer, self.weights['w'+str(i+1)]), self.weights['b'+str(i+1)]))\n",
    "            \n",
    "        self.reconstruction=tf.add(tf.matmul(layer, self.weights['w'+str(self.layernum)]), self.weights['b'+str(self.layernum)])\n",
    "\n",
    "    def partial_fit(self, X, Y):\n",
    "        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X, self.y: Y})\n",
    "        return cost\n",
    "\n",
    "    def calc_total_cost(self, X,Y=None,batch=2048):\n",
    "        if Y is None:\n",
    "            Y=X\n",
    "        cost=0\n",
    "        start=0\n",
    "        for i in range(int(len(X)/batch)):\n",
    "            if start+batch >= len(X):\n",
    "                start=0\n",
    "            start+=batch\n",
    "            batch_xs = X[start:(start + batch)]\n",
    "            batch_ys = Y[start:(start + batch)]\n",
    "            cost+=self.sess.run(self.cost, feed_dict = {self.x: batch_xs,self.y: batch_ys})\n",
    "        return cost\n",
    "\n",
    "    def encode(self, X):\n",
    "        return self.sess.run(self.encoded, feed_dict={self.x: X})\n",
    "\n",
    "    def decode(self, encoded = None):\n",
    "        if encoded is None:\n",
    "            encoded = np.random.normal(size=self.weights[\"b1\"])\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.encoded: encoded})\n",
    "\n",
    "    def reconstruct(self, X,batch=512):\n",
    "        start=0\n",
    "        reconstructionl=np.zeros([int(len(X)),self.charnum*self.maxlen])\n",
    "        for i in range(int(len(X)/batch)+1):\n",
    "            if start+batch >= len(X):\n",
    "                batch_xs = X[start:]\n",
    "                start=0\n",
    "            else:\n",
    "                batch_xs = X[start:(start + batch)]\n",
    "                start+=batch\n",
    "            leng=len(batch_xs)\n",
    "            cur=self.sess.run(self.reconstruction, feed_dict = {self.x: batch_xs})\n",
    "            reconstructionl[i*batch:i*batch+leng,:]=(np.reshape(cur,(leng,self.charnum*self.maxlen)))\n",
    "        if len(X)<batch:\n",
    "            return self.recon(X)\n",
    "        return reconstructionl\n",
    "    \n",
    "    def recon(self,X):\n",
    "        return self.sess.run(self.reconstruction, feed_dict = {self.x: X})\n",
    "    \n",
    "    def reconstruction_accuracy(self,dataX,dataY=None):\n",
    "        \"\"\"\n",
    "        @return char accuracy, word accuracy, levenshtein avg error\n",
    "        \"\"\"\n",
    "        if dataY is None:\n",
    "            dataY=dataX\n",
    "            \n",
    "        accuracy_max=len(data)*self.maxlen\n",
    "        char_accuracy=len(data)*self.maxlen\n",
    "        word_accuracy=len(data)\n",
    "        \n",
    "        levenshtein_error_sum=0\n",
    "        \n",
    "        reconstructed=self.reconstruct(dataX)\n",
    "        \n",
    "        for i in range(len(dataX)):\n",
    "            a=dataY[i].reshape((self.maxlen,self.charnum))\n",
    "            b=reconstructed[i].reshape((self.maxlen,self.charnum))\n",
    "            \n",
    "            word_ok=True\n",
    "            \n",
    "            for j in range(self.maxlen):\n",
    "                if (a[j,:].argmax()!=b[j,:].argmax()):\n",
    "                    char_accuracy-=1\n",
    "                    word_ok=False\n",
    "            if (not word_ok):\n",
    "                word_accuracy-=1   \n",
    "                \n",
    "            levenshtein_error_sum+=Levenshtein.distance(\n",
    "                self.corpus.defeaturize_data_charlevel_onehot([a])[0],\n",
    "                self.corpus.defeaturize_data_charlevel_onehot([b])[0])\n",
    "                \n",
    "        characc=char_accuracy/accuracy_max\n",
    "        wordacc=word_accuracy/len(data)\n",
    "\n",
    "        return characc,wordacc,levenshtein_error_sum/len(data)\n",
    "    \n",
    "    def train(self,\n",
    "              Y_train,X_valid,X_test,\n",
    "              batch_size=512,max_epochs=50,\n",
    "              X_train=None, Y_valid=None, Y_test=None):\n",
    "        \n",
    "        if X_train is None:\n",
    "            X_train = Y_train\n",
    "        if Y_valid is None:\n",
    "            Y_valid = X_valid\n",
    "        if Y_test is None:\n",
    "            Y_test = X_test\n",
    "            \n",
    "        breaker=False\n",
    "        validlog=collections.deque(maxlen=30)\n",
    "        self.logger.logline(\"train.log\",[\"START\"])\n",
    "        self.logger.logline(\"train.log\",[\"config\"]+self.layerlist)\n",
    "        total_batch = int(max_epochs*len(X_train) / batch_size)\n",
    "        # Loop over all batches\n",
    "        start=0\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            start+=batch_size\n",
    "            if start+batch_size >= len(X_train):\n",
    "                start=0\n",
    "            batch_xs = X_train[start:(start + batch_size)]\n",
    "            batch_ys = Y_train[start:(start + batch_size)]\n",
    "            #print(batch_xs.shape,batch_ys.shape)\n",
    "            cost = self.partial_fit(batch_xs,batch_ys)\n",
    "            #avg_cost += cost/ batch_size\n",
    "            if i % self.display_step==0:\n",
    "                validloss=self.calc_total_cost(X=X_valid,Y=Y_valid)\n",
    "                #early stop\n",
    "                self.logger.logline(\"train.log\",[\"batch\",i,\"valid_loss\",validloss])\n",
    "                validlog.append(validloss)\n",
    "\n",
    "                if len(validlog)>20:\n",
    "                    breaker=True\n",
    "                    for j in range(10):\n",
    "                         if validlog[-j]<validlog[-10-j]:\n",
    "                            breaker=False\n",
    "                else:\n",
    "                    breaker=False\n",
    "\n",
    "                if breaker:\n",
    "                    self.logger.logline(\"early_stop.log\",[\"STOPPED\"])\n",
    "                    self.logger.logline(\"early_stop.log\",[\"survived\",i])\n",
    "                    self.logger.logline(\"early_stop.log\",[\"config\"]+self.layerlist)\n",
    "                    self.logger.logline(\"early_stop.log\",[\"train_cost\",self.calc_total_cost(X_train)])\n",
    "                    self.logger.logline(\"early_stop.log\",[\"valid_last_results\"]+list(testlog))\n",
    "                    break\n",
    "        self.logger.logline(\"train.log\",[\"STOP\"])\n",
    "        #train_loss,test_loss,train_char_acc,train_word_acc,test_char_acc,test_word_acc,config\n",
    "\n",
    "        Vchar,Vword,Vleven=self.reconstruction_accuracy(dataX=X_valid,dataY=Y_valid)\n",
    "        Testchar,Testword,Testleven=self.reconstruction_accuracy(dataX=X_test,dataY=Y_test)\n",
    "        Trainchar,Trainword,Trainleven=self.reconstruction_accuracy(dataX=X_train,dataY=Y_train)\n",
    "        \n",
    "        self.logger.logline(\"accuracy.log\",\n",
    "                            [self.calc_total_cost(X_train),\n",
    "                             self.calc_total_cost(X_valid),\n",
    "                             self.calc_total_cost(X_test),\n",
    "                             Trainchar,\n",
    "                             Trainword,\n",
    "                             Vchar,\n",
    "                             Vword,\n",
    "                             Testchar,\n",
    "                             Testword,\n",
    "                             Trainleven,\n",
    "                             Vleven,\n",
    "                             Testleven]+self.layerlist)                  \n",
    "    \n",
    "    def xavier_init(self,fan_in, fan_out, constant = 1):\n",
    "        low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        return tf.random_uniform((fan_in, fan_out),\n",
    "                                 minval = low, maxval = high,\n",
    "                                 dtype = tf.float32)\n",
    "    def save(self,path):\n",
    "        self.saver.save(self.sess, path)\n",
    "        \n",
    "    def load(self,path):\n",
    "        self.saver.restore(self.sess, path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import Levenshtein\n",
    "\n",
    "class Autoencoder_FFNN(Autoencoder):\n",
    "    def __init__(self,\n",
    "                 tf_session, inputdim,\n",
    "                 logger,\n",
    "                 layerlist,\n",
    "                 encode_index,\n",
    "                 corpus,\n",
    "                 optimizer = tf.train.AdamOptimizer(),\n",
    "                 nonlinear=tf.nn.sigmoid,\n",
    "                 disp_step=30,\n",
    "                charnum=0,\n",
    "                maxlen=0):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.charnum=charnum\n",
    "        self.maxlen=maxlen\n",
    "        self.corpus=corpus\n",
    "        self.logger=logger\n",
    "        \n",
    "        self.layerlist=layerlist\n",
    "        self.layernum=len(layerlist)\n",
    "        self.n_input = inputdim\n",
    "        self.encode_index=encode_index\n",
    "        self.display_step=disp_step\n",
    "        self.nonlinear=nonlinear\n",
    "          \n",
    "        self.create_graph()\n",
    "        \n",
    "        self.size=0\n",
    "        nums=[self.n_input,layerlist]\n",
    "        for i in range(1,len(nums)):\n",
    "            self.size+=4*layerlist[i]*layerlist[i-1]\n",
    "        \n",
    "\n",
    "    def create_graph(self):\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights\n",
    "        self._create_layers()\n",
    "        # cost\n",
    "        self.cost =  0.5*tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.y), 2.0))\n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf_session\n",
    "        self.sess.run(init)\n",
    "        self.saver = tf.train.Saver()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "        \n",
    "        all_weights['w1']=tf.Variable(self.xavier_init(self.n_input, self.layerlist[0]))\n",
    "        all_weights['b1'] = tf.Variable(tf.random_normal([self.layerlist[0]], dtype=tf.float32))\n",
    "        \n",
    "        for i in range(1,self.layernum):\n",
    "            all_weights['w'+str(i+1)]=tf.Variable(self.xavier_init(self.layerlist[i-1], self.layerlist[i]))\n",
    "            all_weights['b'+str(i+1)] = tf.Variable(tf.random_normal([self.layerlist[i]], dtype=tf.float32))\n",
    "\n",
    "        return all_weights\n",
    "    \n",
    "    def _create_layers(self,nonlinearity):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        layer=nonlinearity(tf.add(tf.matmul(self.x, self.weights['w1']), self.weights['b1']))\n",
    "        self.encoded=layer\n",
    "        for i in range(1,self.layernum-1):\n",
    "            if i==self.encode_index:\n",
    "                self.encoded=layer\n",
    "            layer=nonlinearity(tf.add(tf.matmul(layer, self.weights['w'+str(i+1)]), self.weights['b'+str(i+1)]))\n",
    "            \n",
    "        self.reconstruction=tf.add(tf.matmul(layer, self.weights['w'+str(self.layernum)]), self.weights['b'+str(self.layernum)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'vahun'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-171ebffe2c2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvahun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSplitBrainCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvahun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'vahun'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from vahun.Text import SplitBrainCorpus\n",
    "import numpy as np\n",
    "from vahun.tools import Timer\n",
    "from vahun.tools import explog\n",
    "#from vahun.autoencoder import Autoencoder_ffnn\n",
    "from vahun.tools import show_performance\n",
    "from vahun.genetic import Settings\n",
    "from vahun.tools import get_reconstruction\n",
    "timer=Timer()\n",
    "\n",
    "\n",
    "\n",
    "corpus_path='/mnt/store/velkey/mnsz2/brain_split.200k.maxlen20'\n",
    "encode=200\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "Xcorpus=SplitBrainCorpus(corpus_path=corpus_path,col=0,size=0)\n",
    "\n",
    "Ycorpus=SplitBrainCorpus(corpus_path=corpus_path,col=1,size=0)\n",
    "\n",
    "logger=explog(encoder_type=\"demo_autoencoder_splitbrain\"+str(encode),\n",
    "              encoding_dim=encode,\n",
    "              feature_len=20,\n",
    "              lang=corpus_path,\n",
    "              unique_words=len(set(Xcorpus.wordlist)),\n",
    "              name=\"demo_autoencoder_top_splitbrain\"+str(encode),\n",
    "              population_size=0,\n",
    "              words=len(Xcorpus.wordlist))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
