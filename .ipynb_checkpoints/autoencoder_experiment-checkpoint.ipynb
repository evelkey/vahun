{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import scipy\n",
    "from corpus import Corpus\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus initalized, fields: ['unique', 'lower', 'hun_lower', 'lower_unique', 'hun_lower_unique'] \n",
      "Unique words:  531\n"
     ]
    }
   ],
   "source": [
    "corp_path='/home/velkey/corp/webkorpusz.wpl'\n",
    "corp=Corpus(corpus_path=corp_path,language=\"Hun\",size=1000,encoding_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_features=corp.featurize_data_charlevel_onehot(corp.hun_lower)\n",
    "train=all_features[0:int(len(all_features)*0.8)]\n",
    "test=all_features[int(len(all_features)*0.8):len(all_features)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635, 360)\n"
     ]
    }
   ],
   "source": [
    "x_train = train.reshape((len(train), np.prod(train.shape[1:])))\n",
    "x_test = test.reshape((len(test), np.prod(test.shape[1:])))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import random\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self,x_train,x_test,y_train,y_test,layer_intervals,encoder_index,optimizer,lossmethod,step_size=0):\n",
    "        self.layernum=len(layer_intervals)\n",
    "        self.layer_intervals=layer_intervals\n",
    "        self.encoder_index=encoder_index\n",
    "        self.optimizer=optimizer\n",
    "        self.lossmethod=loss\n",
    "        self.tried_list=[]\n",
    "        self.train_losses=[]\n",
    "        self.test_losses=[]\n",
    "        self.x_train=x_train\n",
    "        self.y_train=y_train\n",
    "        self.train_len=len(x_train)\n",
    "        self.test_len=len(x_test)\n",
    "        self.x_test=x_test\n",
    "        self.y_test=y_test\n",
    "        \n",
    "        self.data_dim=x_train[0].shape[0]*x_train[0].shape[1]\n",
    "        \n",
    "        \n",
    "    def gen_model(self,layer_data,type):\n",
    "        \"\"\"\n",
    "        @layer_data: [[size,activation],[size,activation]] with the last layer\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def show_words(predict_base,num=30):\n",
    "        encoded_text=encoder.predict(predict_base)\n",
    "        decoded_text = decoder.predict(encoded_text)\n",
    "        for i in range(num):\n",
    "            x=random.randint(0,len(predict_base)-1)\n",
    "            print(\"original:\\t\",corp.defeaturize_data_charlevel_onehot([predict_base[x].reshape(10,36)]),\\\n",
    "                  \"\\tdecoded:\\t\",corp.defeaturize_data_charlevel_onehot([decoded_text[x].reshape(10,36)]))\n",
    "            \n",
    "    def plot_words_as_img():\n",
    "        \n",
    "        encoded_imgs=encoder.predict(x_train)\n",
    "        decoded_imgs = decoder.predict(encoded_imgs)\n",
    "        n = 6  # how many digits we will display\n",
    "        plt.figure(figsize=(21, 4))\n",
    "        \n",
    "        for i in range(n):\n",
    "            # display original\n",
    "            ax = plt.subplot(2, n, i + 1)\n",
    "            plt.imshow(x_test[i].reshape(10, 36))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "            # display reconstruction\n",
    "            ax = plt.subplot(2, n, i + 1 + n)\n",
    "            plt.imshow(decoded_imgs[i].reshape(10,36))\n",
    "\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant = 1):\n",
    "    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out),\n",
    "                             minval = low, maxval = high,\n",
    "                             dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "enc\n",
      "Epoch: 0001 cost= 21957.349269318\n",
      "Epoch: 0002 cost= 22635.015219886\n",
      "Epoch: 0003 cost= 581790.643934659\n",
      "Epoch: 0004 cost= 67969.046844318\n",
      "Epoch: 0005 cost= 274192.158020455\n",
      "Epoch: 0006 cost= 38342.413570455\n",
      "Epoch: 0007 cost= 391874.742343182\n",
      "Epoch: 0008 cost= 20391.960996591\n",
      "Epoch: 0009 cost= 111860.517071591\n",
      "Epoch: 0010 cost= 6568037.921059083\n",
      "Epoch: 0011 cost= 1475194.926781819\n",
      "Epoch: 0012 cost= 91551.042320455\n",
      "Epoch: 0013 cost= 20867.567109091\n",
      "Epoch: 0014 cost= 41782.608554546\n",
      "Epoch: 0015 cost= 24635.806288636\n",
      "Epoch: 0016 cost= 27893.548127273\n",
      "Epoch: 0017 cost= 19526.183731818\n",
      "Epoch: 0018 cost= 74717.278070455\n",
      "Epoch: 0019 cost= 82418.991503409\n",
      "Epoch: 0020 cost= 81133.035793182\n",
      "Total cost: 6.17836e+06\n"
     ]
    }
   ],
   "source": [
    "class Autoencoder():\n",
    "\n",
    "    def __init__(self, featurelen,length,layerlist,encode_index,optimizer = tf.train.AdamOptimizer()):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.layerlist=layerlist\n",
    "        self.layernum=len(layerlist)\n",
    "        self.n_input = featurelen*length\n",
    "        self.encode_index=encode_index\n",
    "\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights  \n",
    "\n",
    "        self._create_layers()\n",
    "\n",
    "        # cost\n",
    "        self.cost =  0.5*tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))\n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "        \n",
    "        all_weights['w'+str(1)]=tf.Variable(xavier_init(self.n_input, self.layerlist[0][0]))\n",
    "        all_weights['b'+str(1)] = tf.Variable(tf.zeros([self.layerlist[0][0]], dtype=tf.float32))\n",
    "        \n",
    "        for i in range(1,self.layernum):\n",
    "            all_weights['w'+str(i+1)]=tf.Variable(xavier_init(self.layerlist[i-1][0], self.layerlist[i][0]))\n",
    "            all_weights['b'+str(i+1)] = tf.Variable(tf.zeros([self.layerlist[i][0]], dtype=tf.float32))\n",
    "\n",
    "        return all_weights\n",
    "    \n",
    "    def _create_layers(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        layer=(self.layerlist[0][1])(tf.add(tf.matmul(self.x, self.weights['w1']), self.weights['b1']))\n",
    "\n",
    "        for i in range(1,self.layernum):\n",
    "            layer=(self.layerlist[i][1])(tf.add(tf.matmul(layer, self.weights['w'+str(i+1)]), self.weights['b'+str(i+1)]))\n",
    "            if i==self.encode_index:\n",
    "                print(\"enc\")\n",
    "                self.encoded=layer\n",
    "            \n",
    "        self.reconstruction=layer\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X})\n",
    "        return cost\n",
    "\n",
    "    def calc_total_cost(self, X):\n",
    "        return self.sess.run(self.cost, feed_dict = {self.x: X})\n",
    "\n",
    "    def encode(self, X):\n",
    "        return self.sess.run(self.encoded, feed_dict={self.x: X})\n",
    "\n",
    "    def decode(self, encoded = None):\n",
    "        if encoded is None:\n",
    "            encoded = np.random.normal(size=self.weights[\"b1\"])\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.encoded: encoded})\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.x: X})\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import sklearn.preprocessing as prep\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n",
    "X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)\n",
    "\n",
    "n_samples = int(mnist.train.num_examples)\n",
    "training_epochs = 20\n",
    "batch_size = 1280\n",
    "\n",
    "def ekv(e):\n",
    "    return e\n",
    "display_step = 1\n",
    "a=[[784,tf.nn.softplus],[784,ekv]]\n",
    "\n",
    "autoencoder = Autoencoder(28,28,\n",
    "                          layerlist=a,\n",
    "                          encode_index=1,\n",
    "                          optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(n_samples / batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = get_random_block_from_data(X_train, batch_size)\n",
    "\n",
    "        # Fit training using batch data\n",
    "        cost = autoencoder.partial_fit(batch_xs)\n",
    "        # Compute average loss\n",
    "        avg_cost += cost / n_samples * batch_size\n",
    "\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch + 1), \\\n",
    "            \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "print (\"Total cost: \" + str(autoencoder.calc_total_cost(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_block_from_data(data, batch_size):\n",
    "    start_index = np.random.randint(0, len(data) - batch_size)\n",
    "    return data[start_index:(start_index + batch_size)]\n",
    "def standard_scale(X_train, X_test):\n",
    "    preprocessor = prep.StandardScaler().fit(X_train)\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.17742257e-05,   7.10462118e-05,   1.86902704e-04,\n",
       "          7.47414306e-05,   1.60204450e-04,   3.80270394e-05,\n",
       "          5.70372969e-04,   2.44110823e-04,   9.73892456e-05,\n",
       "          1.16818279e-04,   4.11783287e-04,   1.48523744e-04,\n",
       "          8.58303247e-06,   2.34839536e-05,   2.18150617e-05,\n",
       "          5.76956299e-05,   1.27069026e-04,   2.93250559e-05,\n",
       "          2.13838604e-04,   9.11909519e-05,   3.96530668e-04,\n",
       "          2.17890847e-04,   7.41454278e-05,   1.06090643e-04,\n",
       "          7.91518323e-05,   1.35650975e-04,   1.23135615e-04,\n",
       "          3.49341397e-04,   8.26086252e-05,   3.52201430e-04,\n",
       "          3.93389882e-06,   2.54002749e-04,   1.04966001e-10,\n",
       "          1.30525659e-04,   3.44089015e-13,   7.68932613e-15,\n",
       "          3.19043375e-10,   6.67569793e-06,   1.11257833e-07,\n",
       "          2.86101886e-06,   5.36059670e-04,   1.30048880e-04,\n",
       "          2.90866428e-05,   3.14707577e-05,   6.07965512e-06,\n",
       "          1.90734681e-06,   7.54486052e-07,   1.16960486e-11,\n",
       "          4.68561971e-08,   2.08099066e-10,   2.53912567e-05,\n",
       "          8.64230024e-05,   4.06495419e-05,   7.45614467e-04,\n",
       "          1.38630799e-04,   7.39070310e-05,   7.43470329e-04,\n",
       "          1.13361610e-04,   1.31130128e-06,   2.37223667e-05,\n",
       "          6.87810608e-14,   8.03869540e-11,   3.44528758e-07,\n",
       "          1.55840022e-03,   3.68452362e-07,   1.71885025e-04,\n",
       "          3.69548115e-06,   2.49144305e-05,   1.01276592e-03,\n",
       "          5.46067837e-04,   3.58993944e-04,   1.43051045e-06,\n",
       "          6.03374559e-04,   1.98463778e-04,   7.98699057e-06,\n",
       "          2.50189012e-04,   6.55648955e-06,   3.60005579e-05,\n",
       "          1.52471592e-03,   1.19209221e-06,   7.42664326e-14,\n",
       "          1.94993827e-10,   1.58178213e-04,   8.86877751e-05,\n",
       "          1.00726778e-04,   7.63005926e-04,   4.16031762e-05,\n",
       "          1.62635465e-12,   2.32771793e-08,   5.00677743e-06,\n",
       "          3.81468999e-06,   4.38146153e-03,   1.87792769e-03,\n",
       "          1.75286597e-03,   1.18606207e-04,   3.94743256e-04,\n",
       "          1.11219997e-03,   2.23890245e-02,   2.54598621e-04,\n",
       "          1.01217045e-03,   9.21445389e-05,   6.50194474e-04,\n",
       "          6.40133367e-05,   4.55369118e-05,   3.49277107e-05,\n",
       "          5.90984186e-04,   4.16031762e-05,   2.20632050e-04,\n",
       "          1.66892869e-06,   4.57753194e-05,   1.37374467e-09,\n",
       "          2.52720492e-05,   5.18546949e-05,   3.68349938e-05,\n",
       "          1.43050129e-05,   1.96894447e-08,   4.51189140e-03,\n",
       "          2.51528436e-05,   7.27174029e-06,   1.35293390e-04,\n",
       "          1.60931249e-05,   1.54970876e-05,   1.62727023e-08,\n",
       "          1.12834996e-07,   1.43051045e-06,   1.54126063e-03,\n",
       "          9.89388136e-05,   2.10263082e-04,   5.72202953e-06,\n",
       "          1.45062870e-07,   4.34144027e-08,   2.62260096e-06,\n",
       "          8.58303247e-06,   4.57753194e-05,   7.79682363e-04,\n",
       "          1.51622720e-04,   5.48361231e-06,   1.14434370e-04,\n",
       "          2.62421658e-08,   3.64773769e-05,   2.28855642e-04,\n",
       "          2.64371251e-04,   7.46089412e-09,   3.93389882e-06,\n",
       "          8.32046208e-05,   7.74857381e-06,   3.00402899e-05,\n",
       "          2.47952248e-05,   1.19209221e-06,   9.10095110e-09,\n",
       "          4.57934302e-10,   3.68678021e-09,   1.19209221e-06,\n",
       "          3.45706349e-06,   3.29012219e-05,   3.70734015e-05,\n",
       "          2.38418306e-06,   1.89322023e-07,   8.10619895e-06,\n",
       "          2.50339190e-06,   2.40799873e-05,   4.40427925e-07,\n",
       "          6.64398101e-07,   7.20176985e-08,   1.51394652e-05,\n",
       "          9.89432192e-06,   2.74180979e-06,   1.63782751e-10,\n",
       "          3.54465592e-04,   3.59595132e-14,   9.92225679e-08,\n",
       "          5.03272843e-03,   3.81302135e-03,   8.10533005e-04,\n",
       "          3.34972501e-05,   1.19209221e-06,   7.03762225e-07,\n",
       "          5.22620951e-08,   3.33780445e-05,   5.02620498e-03,\n",
       "          1.23203658e-02,   9.66158044e-03,   2.25337595e-03,\n",
       "          7.17144285e-04,   9.66320280e-04,   9.00697254e-04,\n",
       "          8.24230898e-04,   3.48983885e-04,   2.50339190e-06,\n",
       "          9.91121141e-08,   7.61686806e-07,   2.02655588e-06,\n",
       "          1.66891605e-05,   9.21445389e-05,   7.74857381e-06,\n",
       "          1.64507474e-05,   1.96437613e-04,   7.51015705e-06,\n",
       "          7.43838318e-05,   4.03905362e-02,   2.23184556e-01,\n",
       "          4.48076464e-02,   4.99007590e-02,   6.84873611e-02,\n",
       "          9.06872842e-03,   1.16970409e-02,   9.66193527e-03,\n",
       "          1.14451818e-01,   9.36953425e-02,   5.05648740e-02,\n",
       "          1.62108213e-01,   8.18097368e-02,   1.94526333e-02,\n",
       "          4.68626358e-02,   6.12986973e-03,   1.96318433e-04,\n",
       "          2.97324306e-07,   6.04589559e-07,   7.27174029e-06,\n",
       "          1.07288304e-06,   8.67364406e-07,   1.78813775e-06,\n",
       "          1.78812334e-05,   1.71659904e-05,   1.09666529e-04,\n",
       "          1.07288304e-06,   2.05277689e-02,   5.29731560e+00,\n",
       "          4.60208178e+00,   4.55806160e+00,   3.80704021e+00,\n",
       "          2.96306777e+00,   2.10579610e+00,   1.08388734e+00,\n",
       "          1.66602266e+00,   2.11259174e+00,   1.52447164e+00,\n",
       "          1.29512298e+00,   1.49390805e+00,   1.37226391e+00,\n",
       "          1.07448769e+00,   4.49781656e-01,   4.38145660e-02,\n",
       "          2.24327371e-02,   4.64915138e-06,   9.05986508e-06,\n",
       "          1.45305574e-04,   3.32852153e-07,   1.88207167e-07,\n",
       "          3.30204275e-05,   1.49010502e-05,   2.03928324e-12,\n",
       "          2.39607798e-05,   8.03438306e-05,   1.46421461e+01,\n",
       "          1.24079933e+01,   6.95331621e+00,   5.38362408e+00,\n",
       "          3.54888248e+00,   1.77125108e+00,   2.27540064e+00,\n",
       "          1.32250166e+00,   1.08240056e+00,   1.74450922e+00,\n",
       "          1.78876674e+00,   1.78379345e+00,   1.44343162e+00,\n",
       "          1.15983105e+00,   1.29756153e+00,   1.59988213e+00,\n",
       "          4.65792209e-01,   7.30656236e-02,   3.10189743e-03,\n",
       "          1.58494210e-03,   5.16162909e-05,   2.57725077e-07,\n",
       "          6.55648955e-06,   7.18691922e-07,   2.94522790e-04,\n",
       "          5.10473264e-09,   2.29570738e-04,   6.07965512e-06,\n",
       "          2.02539330e+01,   1.24516983e+01,   6.98679638e+00,\n",
       "          4.34339714e+00,   3.01458907e+00,   2.14185405e+00,\n",
       "          8.11566651e-01,   1.32901430e-01,   3.22122276e-02,\n",
       "          8.72813910e-02,   2.66834367e-02,   1.94999818e-02,\n",
       "          1.35435071e-02,   5.32368779e-01,   1.85505950e+00,\n",
       "          1.63898468e+00,   1.10827613e+00,   1.24230635e+00,\n",
       "          4.79417853e-02,   2.80338172e-02,   6.75893825e-05,\n",
       "          1.61396354e-04,   2.31263348e-05,   8.29662240e-05,\n",
       "          1.54970876e-05,   2.59202620e-12,   1.21131178e-11,\n",
       "          3.01846815e-03,   2.94555497e+00,   3.45147729e+00,\n",
       "          8.22358549e-01,   2.01982576e-02,   8.50792276e-04,\n",
       "          3.59211140e-03,   4.57870169e-03,   6.62310235e-03,\n",
       "          6.85576000e-04,   2.51499965e-04,   2.36721622e-04,\n",
       "          1.29810491e-04,   4.05252259e-03,   1.64314900e-02,\n",
       "          3.36070433e-02,   4.66257930e-01,   1.11825705e+00,\n",
       "          1.95473325e+00,   2.90711522e-01,   6.01150049e-03,\n",
       "          1.21592739e-05,   3.09943675e-06,   1.16818279e-04,\n",
       "          2.16460641e-04,   1.57355025e-05,   4.36706715e-09,\n",
       "          3.73217895e-18,   6.85099399e-09,   9.65590607e-06,\n",
       "          2.94988952e-03,   3.92357679e-03,   4.74697910e-04,\n",
       "          1.90734681e-06,   1.74387984e-04,   9.87042440e-04,\n",
       "          1.52126409e-03,   7.31915527e-04,   7.08448235e-04,\n",
       "          4.57539776e-04,   2.20008893e-03,   1.02401590e-02,\n",
       "          1.38770568e-03,   6.84701372e-03,   1.08532444e-01,\n",
       "          1.40048361e+00,   1.40182483e+00,   1.58040595e+00,\n",
       "          2.27278359e-02,   1.76863125e-07,   3.76763580e-07,\n",
       "          1.32321438e-05,   3.69541958e-05,   3.09943675e-06,\n",
       "          8.18934277e-05,   4.78017828e-05,   1.42756612e-10,\n",
       "          4.54440432e-08,   6.97350042e-05,   4.76842659e-04,\n",
       "          1.26115468e-04,   4.88756905e-06,   1.53778801e-05,\n",
       "          1.50202577e-05,   6.86648127e-04,   2.22896531e-04,\n",
       "          1.27553130e-05,   5.35235376e-05,   5.16996253e-04,\n",
       "          1.99059694e-04,   2.87610659e-04,   2.61833915e-03,\n",
       "          2.04791039e-01,   1.49532986e+00,   1.77805030e+00,\n",
       "          2.62857413e+00,   6.30954385e-01,   1.69858817e-04,\n",
       "          4.17231649e-06,   9.65590607e-06,   1.33513513e-05,\n",
       "          1.78813775e-06,   2.83714089e-05,   1.08479862e-05,\n",
       "          9.47668959e-05,   1.44512256e-12,   1.36485323e-04,\n",
       "          1.03253440e-03,   1.87156838e-05,   1.23612394e-04,\n",
       "          3.45706349e-06,   4.13647685e-05,   2.34662113e-03,\n",
       "          9.54820789e-05,   1.37609845e-07,   2.84906146e-05,\n",
       "          4.41073416e-06,   1.72004206e-04,   3.82469472e-04,\n",
       "          1.76813185e-01,   4.84848320e-01,   1.39622700e+00,\n",
       "          1.32447326e+00,   2.19822168e+00,   4.95706737e-01,\n",
       "          5.52978192e-04,   1.57355025e-05,   1.07287788e-05,\n",
       "          8.71679845e-07,   5.36440348e-06,   1.69275754e-05,\n",
       "          5.50731747e-05,   2.50339190e-06,   1.66892869e-06,\n",
       "          1.38281821e-05,   1.76056623e-04,   9.65548606e-05,\n",
       "          1.23969978e-04,   1.02395534e-04,   5.44643588e-02,\n",
       "          2.23077671e-03,   2.34839536e-05,   4.07911697e-03,\n",
       "          1.34650609e-02,   1.27080396e-01,   9.22856256e-02,\n",
       "          4.56184268e-01,   1.30234635e+00,   1.19610310e+00,\n",
       "          2.06365132e+00,   1.64100003e+00,   1.66122365e+00,\n",
       "          1.30642414e+00,   7.02118050e-05,   7.64109831e-10,\n",
       "          1.13230506e-07,   5.30467260e-05,   3.33785465e-06,\n",
       "          1.19005833e-10,   2.64641112e-05,   3.57148732e-14,\n",
       "          1.54971951e-06,   2.38032619e-04,   1.56628739e-04,\n",
       "          4.04157006e-04,   2.24849931e-03,   2.07819827e-02,\n",
       "          1.18085518e-01,   2.88554072e-01,   2.64670014e-01,\n",
       "          9.85518038e-01,   4.88065302e-01,   1.59228396e+00,\n",
       "          6.83167279e-01,   6.65208101e-01,   8.38594854e-01,\n",
       "          4.72732872e-01,   1.54040706e+00,   2.25719976e+00,\n",
       "          1.75666809e+00,   7.45460391e-01,   7.39301089e-04,\n",
       "          3.01643321e-10,   2.65833187e-05,   6.49000782e-08,\n",
       "          1.78813775e-06,   1.81321822e-13,   1.41858045e-05,\n",
       "          5.97932537e-10,   6.55648955e-06,   2.80137901e-05,\n",
       "          3.23475176e-03,   4.57514171e-03,   6.47617280e-02,\n",
       "          3.36645156e-01,   1.07731032e+00,   8.82406890e-01,\n",
       "          1.22338331e+00,   1.41601193e+00,   2.19784856e+00,\n",
       "          1.67987096e+00,   9.51202393e-01,   2.12397620e-01,\n",
       "          6.72616661e-02,   4.30479832e-02,   9.14283991e-01,\n",
       "          1.65039849e+00,   2.28620791e+00,   7.38251626e-01,\n",
       "          2.77480722e-04,   7.99954325e-10,   2.17636668e-07,\n",
       "          5.00677743e-06,   2.50339190e-06,   2.76384610e-10,\n",
       "          3.66382243e-04,   9.07808868e-08,   2.97978811e-04,\n",
       "          5.12586812e-05,   1.39330579e-02,   1.71375368e-02,\n",
       "          1.02036428e+00,   1.90826416e+00,   1.72052193e+00,\n",
       "          3.80894452e-01,   3.79349709e-01,   4.99687731e-01,\n",
       "          1.10230625e-01,   8.49839416e-04,   1.19609293e-02,\n",
       "          1.39894858e-02,   4.95571904e-02,   7.39799486e-03,\n",
       "          4.99032378e-01,   1.72392786e+00,   1.96537471e+00,\n",
       "          7.29180336e-01,   1.72537647e-03,   1.96213623e-08,\n",
       "          3.92020496e-08,   2.07422017e-05,   4.94706328e-05,\n",
       "          9.50737405e-11,   1.02519462e-05,   1.31130128e-06,\n",
       "          4.86238906e-03,   5.33087179e-03,   9.67197027e-03,\n",
       "          6.54055830e-03,   3.77622068e-01,   8.51710677e-01,\n",
       "          3.90467286e-01,   3.80171984e-02,   1.84333362e-02,\n",
       "          1.04884200e-01,   3.58605362e-03,   7.63125019e-04,\n",
       "          1.16905607e-02,   1.11569120e-02,   8.16105399e-03,\n",
       "          3.18377488e-03,   1.34681833e+00,   1.85108495e+00,\n",
       "          1.99256825e+00,   5.19868374e-01,   7.84366348e-05,\n",
       "          2.05037868e-05,   1.89540970e-05,   1.78813775e-06,\n",
       "          6.86886371e-04,   1.62123324e-05,   8.34461571e-06,\n",
       "          1.98339848e-07,   2.95607001e-03,   2.51815189e-02,\n",
       "          7.79325934e-03,   2.47952248e-05,   5.81110595e-03,\n",
       "          2.40441947e-03,   6.47811801e-04,   3.43978842e-04,\n",
       "          1.86902704e-04,   3.63283907e-04,   5.58816188e-04,\n",
       "          3.68570723e-03,   2.24872883e-02,   9.41503514e-03,\n",
       "          4.05501574e-03,   1.23030767e-01,   1.41154170e+00,\n",
       "          1.35361564e+00,   1.92585480e+00,   1.42523766e-01,\n",
       "          7.90326376e-05,   4.33231908e-04,   2.10024707e-04,\n",
       "          6.99752069e-04,   1.87156838e-05,   5.72202953e-06,\n",
       "          9.00708130e-08,   1.45166808e-07,   1.13719194e-04,\n",
       "          5.13769360e-03,   7.16769695e-03,   7.04874459e-04,\n",
       "          9.50052927e-05,   1.54066342e-07,   1.20863927e-11,\n",
       "          1.19209221e-06,   4.12455629e-05,   5.84074121e-04,\n",
       "          1.53471425e-03,   2.13216757e-03,   2.49598594e-03,\n",
       "          6.67153578e-03,   2.70774178e-02,   6.32282317e-01,\n",
       "          2.00256848e+00,   2.56544995e+00,   4.24357414e-01,\n",
       "          2.76865941e-02,   5.44876384e-04,   1.63065008e-04,\n",
       "          8.14582861e-04,   1.87156838e-05,   4.05310766e-06,\n",
       "          6.86621934e-05,   8.48002646e-09,   1.03711545e-05,\n",
       "          3.62733565e-02,   1.75119862e-02,   6.94125239e-03,\n",
       "          1.72448717e-02,   9.80442669e-03,   1.37081282e-04,\n",
       "          4.78374389e-08,   1.43051045e-06,   1.67475024e-04,\n",
       "          1.23135615e-04,   2.09612353e-03,   1.96675988e-04,\n",
       "          5.04009076e-04,   1.52974669e-02,   3.91799301e-01,\n",
       "          1.66385603e+00,   2.68135738e+00,   2.30015039e+00,\n",
       "          2.03967869e-01,   9.37618199e-04,   1.90734681e-06,\n",
       "          1.18016496e-05,   1.17056668e-04,   2.74739665e-04,\n",
       "          4.19607895e-05,   2.93250559e-05,   1.00907416e-03,\n",
       "          5.23315102e-05,   7.32007288e-10,   2.20080256e-03,\n",
       "          1.73192169e-03,   2.04151962e-03,   1.95026828e-03,\n",
       "          3.59282410e-03,   1.63315399e-05,   5.24519510e-06,\n",
       "          4.12581151e-08,   1.10500907e-04,   1.19900319e-03,\n",
       "          1.28437020e-03,   5.36359800e-03,   5.69097325e-02,\n",
       "          1.19816923e+00,   2.29123449e+00,   2.89186072e+00,\n",
       "          1.45041943e+00,   8.88867769e-03,   3.35517863e-04,\n",
       "          4.55369118e-05,   1.36353949e-03,   1.10207847e-03,\n",
       "          1.07288304e-06,   2.25303011e-05,   5.24519510e-06,\n",
       "          1.67713399e-04,   3.49698908e-04,   1.06095704e-05,\n",
       "          1.95126573e-04,   1.98221263e-02,   6.63340511e-03,\n",
       "          2.86906259e-03,   1.32612130e-02,   6.43999607e-04,\n",
       "          4.80401904e-05,   1.54971951e-06,   1.22897225e-04,\n",
       "          4.57753194e-05,   1.86424423e-03,   1.53667303e-02,\n",
       "          1.38315678e+00,   3.90095901e+00,   4.52093887e+00,\n",
       "          2.05184698e+00,   4.76413034e-02,   2.06043795e-02,\n",
       "          1.17792818e-03,   1.40593098e-02,   5.79593517e-03,\n",
       "          1.54960071e-04,   9.17907346e-06,   1.10864021e-05,\n",
       "          3.20667859e-05,   5.12586812e-05,   1.87617814e-04,\n",
       "          2.49395583e-07,   5.48361231e-06,   5.65726426e-04,\n",
       "          3.01585346e-03,   8.38881417e-04,   3.54942254e-04,\n",
       "          2.69495882e-04,   7.43112934e-04,   8.46504408e-04,\n",
       "          9.23829284e-05,   2.26497400e-06,   3.21864559e-06,\n",
       "          8.87237210e-03,   3.60163093e+00,   5.15784883e+00,\n",
       "          2.57425737e+00,   2.19660196e-02,   3.63867288e-03,\n",
       "          3.41522973e-03,   5.62867033e-04,   5.24507122e-05,\n",
       "          5.41195514e-05,   7.74857381e-06,   9.07141512e-05,\n",
       "          4.97090368e-05,   1.12363126e-03,   2.51023244e-04,\n",
       "          6.60397636e-05,   1.80004408e-05,   8.14166269e-05,\n",
       "          1.05345732e-09,   1.53291403e-04,   8.71516881e-04,\n",
       "          4.68144484e-04,   7.88020436e-04,   1.75818248e-04,\n",
       "          5.75019454e-04,   7.95094384e-05,   5.79308544e-04,\n",
       "          1.63278717e-03,   1.36402041e-01,   4.18562984e+00,\n",
       "          2.96599293e+00,   8.32427368e-02,   1.84151740e-03,\n",
       "          1.91550978e-04,   8.14940198e-04,   3.26628106e-05,\n",
       "          5.36440348e-06,   1.76999526e-10,   6.94542135e-17,\n",
       "          9.91947982e-11,   8.16550310e-05,   7.51015705e-06,\n",
       "          6.71757094e-04,   1.15983916e-04,   8.72573946e-05,\n",
       "          4.66097445e-05,   1.09671946e-05,   3.80103657e-11,\n",
       "          1.22784813e-05,   3.49277107e-05,   1.58658708e-07,\n",
       "          1.41858045e-05,   3.84137762e-04,   1.78202012e-04,\n",
       "          1.03027176e-03,   2.17499072e-03,   4.15358110e-04,\n",
       "          4.89697121e-02,   2.51679178e-02,   1.79274706e-04,\n",
       "          3.55237389e-05,   5.55499864e-05,   3.93389882e-06,\n",
       "          2.85500148e-11,   3.96121935e-09,   1.11812064e-04,\n",
       "          3.15735553e-04,   1.65699548e-05,   5.78148356e-05,\n",
       "          1.68786108e-04]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.generate(autoencoder.transform([X_train[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
