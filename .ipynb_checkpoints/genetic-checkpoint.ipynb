{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus initalized, fields: ['unique', 'lower', 'hun_lower', 'lower_unique', 'hun_lower_unique'] \n",
      "Unique words:  25545\n",
      "(60152, 360)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction=1\n",
    "\n",
    "import time\n",
    "import collections\n",
    "import scipy\n",
    "from corpus import Corpus\n",
    "import numpy as np\n",
    "from random import randint, random\n",
    "\n",
    "\n",
    "def xavier_init(fan_in, fan_out, constant = 1):\n",
    "    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out),\n",
    "                             minval = low, maxval = high,\n",
    "                             dtype = tf.float32)\n",
    "\n",
    "corp_path='/home/velkey/corp/webkorpusz.wpl'\n",
    "corp=Corpus(corpus_path=corp_path,language=\"Hun\",size=100000,encoding_len=10)\n",
    "all_features=corp.featurize_data_charlevel_onehot(corp.hun_lower)\n",
    "train=all_features[0:int(len(all_features)*0.8)]\n",
    "test=all_features[int(len(all_features)*0.8):len(all_features)]\n",
    "x_train = train.reshape((len(train), np.prod(train.shape[1:])))\n",
    "x_test = test.reshape((len(test), np.prod(test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.timers=dict()\n",
    "    def add(self,str):\n",
    "        self.timers[str]=time.time()\n",
    "    def get(self,str):\n",
    "        return time.time()-self.timers[str]\n",
    "timer=Timer()\n",
    "\n",
    "def logtsv(array):\n",
    "    string=\"\"\n",
    "    for item in array:\n",
    "        string+=str(item)\n",
    "        string+=\"\\t\"\n",
    "    string=string[0:len(string)-1]+\"\\n\"\n",
    "    with open(\"train.tsv\", \"a\") as myfile:\n",
    "        myfile.write(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder_ffnn():\n",
    "    def __init__(self,experiment,tf_session, inputdim,layerlist,encode_index,optimizer = tf.train.AdamOptimizer(),nonlinear=tf.nn.relu):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.experiment=experiment\n",
    "        \n",
    "        self.layerlist=layerlist\n",
    "        self.layernum=len(layerlist)\n",
    "        self.n_input = inputdim\n",
    "        self.encode_index=encode_index\n",
    "        self.display_step=10\n",
    "\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights  \n",
    "\n",
    "        self._create_layers(nonlinear)\n",
    "\n",
    "        # cost\n",
    "        self.cost =  0.5*tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))\n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf_session\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        self.size=0\n",
    "        nums=[self.n_input,layerlist]\n",
    "        for i in range(1,len(nums)):\n",
    "            self.size+=4*layerlist[i]*layerlist[i-1]\n",
    "        \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "        \n",
    "        all_weights['w1']=tf.Variable(xavier_init(self.n_input, self.layerlist[0]))\n",
    "        all_weights['b1'] = tf.Variable(tf.random_normal([self.layerlist[0]], dtype=tf.float32))\n",
    "        \n",
    "        for i in range(1,self.layernum):\n",
    "            all_weights['w'+str(i+1)]=tf.Variable(xavier_init(self.layerlist[i-1], self.layerlist[i]))\n",
    "            all_weights['b'+str(i+1)] = tf.Variable(tf.random_normal([self.layerlist[i]], dtype=tf.float32))\n",
    "\n",
    "        return all_weights\n",
    "    \n",
    "    def _create_layers(self,nonlinearity=tf.nn.relu):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        layer=nonlinearity(tf.add(tf.matmul(self.x, self.weights['w1']), self.weights['b1']))\n",
    "\n",
    "        for i in range(1,self.layernum-1):\n",
    "            if i==self.encode_index:\n",
    "                self.encoded=layer\n",
    "            layer=nonlinearity(tf.add(tf.matmul(layer, self.weights['w'+str(i+1)]), self.weights['b'+str(i+1)]))\n",
    "            \n",
    "        self.reconstruction=tf.add(tf.matmul(layer, self.weights['w'+str(self.layernum)]), self.weights['b'+str(self.layernum)])\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X})\n",
    "        return cost\n",
    "\n",
    "    def calc_total_cost(self, X):\n",
    "        return self.sess.run(self.cost, feed_dict = {self.x: X})\n",
    "\n",
    "    def encode(self, X):\n",
    "        return self.sess.run(self.encoded, feed_dict={self.x: X})\n",
    "\n",
    "    def decode(self, encoded = None):\n",
    "        if encoded is None:\n",
    "            encoded = np.random.normal(size=self.weights[\"b1\"])\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.encoded: encoded})\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.x: X})\n",
    "    \n",
    "    def train(self,X_train,X_test,batch_size,max_epochs):\n",
    "        breaker=False\n",
    "        testlog=collections.deque(maxlen=30)\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(X_train) / batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs = self.get_random_block_from_data(X_train, batch_size)\n",
    "                cost = self.partial_fit(batch_xs)\n",
    "                avg_cost += cost/ batch_size\n",
    "                \n",
    "                #early stop\n",
    "                testlog.append(self.calc_total_cost(X_test))\n",
    "                for i in range(8):\n",
    "                    if len(testlog)>20 and testlog[-i]>=testlog[-10-i]*0.995:\n",
    "                        breaker=True\n",
    "                    else:\n",
    "                        breaker=False\n",
    "                if breaker:\n",
    "                    print(\"STOPPED OVERFIT\")\n",
    "                    break\n",
    "            # Display logs per epoch step\n",
    "            if epoch % self.display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "            if breaker:\n",
    "                break\n",
    "                \n",
    "    def get_random_block_from_data(self,data, batch_size):\n",
    "        start_index = np.random.randint(0, len(data) - batch_size)\n",
    "        return data[start_index:(start_index + batch_size)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class experiment:\n",
    "    \n",
    "    def __init__(self,out_dim,minw,maxw,encoded_width,layermin=1,layermax=5):\n",
    "        self.len=randint(layermin,layermax)*2\n",
    "        self.weights=[randint(minw,maxw) for n in range(self.len)]\n",
    "        self.weights[int(self.len/2-1)]=encoded_width\n",
    "        self.weights[-1]=out_dim\n",
    "        \n",
    "    def set(self,weights):\n",
    "        self.len=len(weights)\n",
    "        self.weights=weights\n",
    "        \n",
    "class evolution:\n",
    "    \n",
    "    def __init__(self,x_train,x_test,population_size,encoder,dim,repeat_runs=2,epoch=30,batch=512,disp_freq=1):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.encoded_width=encoder\n",
    "        self.dim=dim\n",
    "        self.min=10\n",
    "        self.max=200\n",
    "        self.repeat_runs=repeat_runs\n",
    "        \n",
    "        self.training_epochs = epoch\n",
    "        self.batch_size = batch\n",
    "        self.display_step = disp_freq\n",
    "        self.x_train=x_train\n",
    "        self.x_test=x_test\n",
    "        \n",
    "        self.learnrate=0.001\n",
    "        self.batchsize=512\n",
    "        self.maxepoch=100\n",
    "        self.optimizer=tf.train.AdamOptimizer(learning_rate = self.learnrate)\n",
    "        \n",
    "        \n",
    "        self.retain_p=0.2\n",
    "        self.random_select_p=0.05\n",
    "        self.mutate_p=0.1\n",
    "        self.mutate_len_p=0.1\n",
    "        self.mutate_width_p=0.4\n",
    "        self.population_size=population_size\n",
    "        self.population=self.gen_population(population_size)\n",
    "        \n",
    "        self.target=0\n",
    "        \n",
    "\n",
    "        \n",
    "    def ekv(self,e):\n",
    "        return e\n",
    "       \n",
    "    \n",
    "    def gen_population(self,count):\n",
    "        \"\"\"\n",
    "        count: the number of individuals in the population\n",
    "        \"\"\"\n",
    "        self.sess = tf.Session(config=config)\n",
    "        \n",
    "        population=[]\n",
    "        for x in range(count):\n",
    "            exp=experiment(out_dim=self.dim,minw=self.min,maxw=self.max,encoded_width=self.encoded_width)\n",
    "            population.append(Autoencoder_ffnn(experiment=exp,tf_session=self.sess,inputdim=self.dim,layerlist=exp.weights,\n",
    "                                               encode_index=int(exp.len/2-1),\n",
    "                                               optimizer = self.optimizer))\n",
    "        return population\n",
    "    \n",
    "    def new_generation(self,experiments):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.sess.close()\n",
    "        self.sess = tf.Session(config=config)\n",
    "        \n",
    "        print(\"New generation is being created.\")\n",
    "        \n",
    "        population=[]\n",
    "        for x in range(len(experiments)):\n",
    "    \n",
    "            population.append(Autoencoder_ffnn(experiment=experiments[x],tf_session=self.sess,inputdim=self.dim,layerlist=experiments[x].weights,\n",
    "                                               encode_index=int(experiments[x].len/2-1),\n",
    "                                               optimizer = self.optimizer))\n",
    "        return population\n",
    "    \n",
    "    def train_population(self):\n",
    "        self.population_fitness=[]\n",
    "        for individual in self.population:\n",
    "            sum_cost=0\n",
    "            for i in range(self.repeat_runs): #average the model's fitness\n",
    "                individual.train(self.x_train,x_test,self.batchsize,self.maxepoch)\n",
    "                sum_cost+=individual.calc_total_cost(self.x_test)\n",
    "            self.population_fitness.append(sum_cost/self.repeat_runs)\n",
    "        return self.population_fitness\n",
    "    \n",
    "\n",
    "    def grade(self):\n",
    "        'Find average fitness for a population.'\n",
    "        summed = sum(self.population_fitness)\n",
    "        self.graded= summed / (self.population_size * 1.0)\n",
    "        return self.graded\n",
    "    \n",
    "    def mutate(self,group):\n",
    "        for individual in group:\n",
    "            if self.mutate_p > random():\n",
    "                if self.mutate_len_p>random():\n",
    "                    if random()<0.5:\n",
    "                        individual.len+=2\n",
    "                        individual.weights=[randint(self.min, self.max),randint(self.min, self.max)]+individual.weights\n",
    "                        individual.weights[int(individual.len/2-1)]=self.encoded_width\n",
    "                    else :\n",
    "                        if individual.len!=2:\n",
    "                            individual.len-=2\n",
    "                            individual.weights=individual.weights[2:]\n",
    "                            individual.weights[int(individual.len/2-1)]=self.encoded_width\n",
    "                if self.mutate_width_p>random():\n",
    "                    pos_to_mutate = randint(0,individual.len-2)\n",
    "                    if pos_to_mutate!=int(individual.len/2-1):\n",
    "                        if 0.5>random():\n",
    "                            individual.weights[pos_to_mutate] +=20\n",
    "                        else:\n",
    "                            individual.weights[pos_to_mutate] -=20\n",
    "        self.mutants=group\n",
    "        return group\n",
    "\n",
    "    def evolve(self):\n",
    "        self.train_population()\n",
    "        \n",
    "        #select top individs\n",
    "        graded = [(self.population_fitness[x], self.population[x].experiment) for x in range(self.population_size)]\n",
    "        graded = [ x[1] for x in sorted(graded)]\n",
    "        retain_length = int(len(graded)*self.retain_p)\n",
    "        parents = graded[:retain_length]\n",
    "        \n",
    "        \n",
    "        # randomly add other individuals to\n",
    "        # promote genetic diversity\n",
    "        for individual in graded[retain_length:]:\n",
    "            if self.random_select_p > random():\n",
    "                parents.append(individual)\n",
    "        \n",
    "        # mutate \n",
    "        mutants=self.mutate(parents)\n",
    "       \n",
    "        # crossover parents to create children (aka sex)\n",
    "        mutants_length = len(mutants)\n",
    "        desired_length = self.population_size - mutants_length\n",
    "        children = []\n",
    "        while len(children) < desired_length:\n",
    "            male = randint(0, mutants_length-1)\n",
    "            female = randint(0, mutants_length-1)\n",
    "            if male != female:\n",
    "                male = mutants[male]\n",
    "                female = mutants[female]\n",
    "                \n",
    "                child=experiment(out_dim=self.dim,minw=self.min,maxw=self.max,encoded_width=self.encoded_width)\n",
    "                weights = male.weights[:int(male.len/2-1)]+female.weights[int(female.len/2-1):]\n",
    "                child.set(weights)\n",
    "                children.append(child)\n",
    "                \n",
    "        mutants.extend(children)\n",
    "        \n",
    "        self.population=self.new_generation(mutants)\n",
    "        return mutants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=evolution(x_train,x_test,80,100,360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    for i in range(x.population_size):\n",
    "        print(x.population[i].experiment.weights)\n",
    "        \n",
    "    x.evolve()\n",
    "    print(x.grade())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
