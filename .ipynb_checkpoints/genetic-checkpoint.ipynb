{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus initalized, fields: ['unique', 'lower', 'hun_lower', 'lower_unique', 'hun_lower_unique'] \n",
      "Unique words:  531\n",
      "(635, 360)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "import scipy\n",
    "from corpus import Corpus\n",
    "import numpy as np\n",
    "from random import randint\n",
    "class experiment:\n",
    "    \n",
    "    def __init__(self,out_dim,minw,maxw,encoded_width,layermin=1,layermax=10):\n",
    "        self.len=randint(layermin,layermax)*2+1\n",
    "        self.weights=[randint(minw,maxw) for n in range(self.len)]\n",
    "        self.weights[int(self.len-1/2)]=encoded_width\n",
    "        self.weights[-1]=out_dim\n",
    "def xavier_init(fan_in, fan_out, constant = 1):\n",
    "    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out),\n",
    "                             minval = low, maxval = high,\n",
    "                             dtype = tf.float32)\n",
    "corp_path='/home/velkey/corp/webkorpusz.wpl'\n",
    "corp=Corpus(corpus_path=corp_path,language=\"Hun\",size=1000,encoding_len=10)\n",
    "all_features=corp.featurize_data_charlevel_onehot(corp.hun_lower)\n",
    "train=all_features[0:int(len(all_features)*0.8)]\n",
    "test=all_features[int(len(all_features)*0.8):len(all_features)]\n",
    "x_train = train.reshape((len(train), np.prod(train.shape[1:])))\n",
    "x_test = test.reshape((len(test), np.prod(test.shape[1:])))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder_ffnn():\n",
    "\n",
    "    def __init__(self, featurelen,length,layerlist,encode_index,optimizer = tf.train.AdamOptimizer()):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.layerlist=layerlist\n",
    "        self.layernum=len(layerlist)\n",
    "        self.n_input = featurelen*length\n",
    "        self.encode_index=encode_index\n",
    "        self.display_step=1\n",
    "\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights  \n",
    "\n",
    "        self._create_layers()\n",
    "\n",
    "        # cost\n",
    "        self.cost =  0.5*tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))\n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.sess.run(init)\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "        \n",
    "        all_weights['w'+str(1)]=tf.Variable(xavier_init(self.n_input, self.layerlist[0][0]))\n",
    "        all_weights['b'+str(1)] = tf.Variable(tf.random_normal([self.layerlist[0][0]], dtype=tf.float32))\n",
    "        \n",
    "        for i in range(1,self.layernum):\n",
    "            all_weights['w'+str(i+1)]=tf.Variable(xavier_init(self.layerlist[i-1][0], self.layerlist[i][0]))\n",
    "            all_weights['b'+str(i+1)] = tf.Variable(tf.random_normal([self.layerlist[i][0]], dtype=tf.float32))\n",
    "\n",
    "        return all_weights\n",
    "    \n",
    "    def _create_layers(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        layer=(self.layerlist[0][1])(tf.add(tf.matmul(self.x, self.weights['w1']), self.weights['b1']))\n",
    "\n",
    "        for i in range(1,self.layernum):\n",
    "            layer=(self.layerlist[i][1])(tf.add(tf.matmul(layer, self.weights['w'+str(i+1)]), self.weights['b'+str(i+1)]))\n",
    "            if i==self.encode_index:\n",
    "                print(\"enc\")\n",
    "                self.encoded=layer\n",
    "            \n",
    "        self.reconstruction=layer\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X})\n",
    "        return cost\n",
    "\n",
    "    def calc_total_cost(self, X):\n",
    "        return self.sess.run(self.cost, feed_dict = {self.x: X})\n",
    "\n",
    "    def encode(self, X):\n",
    "        return self.sess.run(self.encoded, feed_dict={self.x: X})\n",
    "\n",
    "    def decode(self, encoded = None):\n",
    "        if encoded is None:\n",
    "            encoded = np.random.normal(size=self.weights[\"b1\"])\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.encoded: encoded})\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        return self.sess.run(self.reconstruction, feed_dict={self.x: X})\n",
    "    \n",
    "    def train(self,X_train,X_test,batch_size,max_epochs):\n",
    "        for epoch in range(max_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(X_train) / batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs = self.get_random_block_from_data(X_train, batch_size)\n",
    "               \n",
    "                cost = self.partial_fit(batch_xs)\n",
    "                \n",
    "                avg_cost += cost/ batch_size\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            if epoch % self.display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch + 1), \\\n",
    "                    \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "        \n",
    "    def get_random_block_from_data(self,data, batch_size):\n",
    "        start_index = np.random.randint(0, len(data) - batch_size)\n",
    "        return data[start_index:(start_index + batch_size)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class evolution:\n",
    "    \n",
    "    def __init__(self,x_train,x_test,population_size,encoder,dim,repeat_runs=2,epoch=30,batch=512,disp_freq=1):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encoded_width=encoder\n",
    "        self.dim=dim\n",
    "        self.min=10\n",
    "        self.max=2000\n",
    "        self.repeat_runs=repeat_runs\n",
    "        self.population_size=population_size\n",
    "        self.population=self.population(population_size)\n",
    "        self.training_epochs = epoch\n",
    "        self.batch_size = batch\n",
    "        self.display_step = disp_freq\n",
    "        self.x_train=x_train\n",
    "        self.x_test=x_test\n",
    "        \n",
    "    def ekv(self,e):\n",
    "        return e\n",
    "       \n",
    "        \n",
    "\n",
    "    def population(self,count):\n",
    "        \"\"\"\n",
    "        count: the number of individuals in the population\n",
    "        \"\"\"\n",
    "        return [ experiment(out_dim=self.dim,minw=self.min,maxw=self.max,encoded_width=self.encoded_width) for x in range(count) ]\n",
    "\n",
    "    def fitness(self,individual, target=0):\n",
    "        \"\"\"\n",
    "        Determine the fitness of an individual. Higher is better.\n",
    "\n",
    "        individual: the individual to evaluate\n",
    "        target: the target number individuals are aiming for\n",
    "        \"\"\"\n",
    "        \n",
    "        sum_cost=0\n",
    "        for i in range(self.repeat_runs):\n",
    "\n",
    "\n",
    "            a=[[360,tf.nn.softplus],[360,self.ekv]]\n",
    "\n",
    "            autoencoder = Autoencoder_ffnn(10,36,\n",
    "                                          layerlist=a,\n",
    "                                          encode_index=1,\n",
    "                                          optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))\n",
    "            autoencoder.train(self.x_train,x_test,512,10)\n",
    "\n",
    "\n",
    "            print (\"Total cost: \" + str(autoencoder.calc_total_cost(self.x_test)))\n",
    "            sum_cost+=autoencoder.calc_total_cost(self.x_test)\n",
    "        return abs(sum_cost/self.repeat_runs)\n",
    "\n",
    "    def grade(self,pop, target):\n",
    "        'Find average fitness for a population.'\n",
    "        summed = sum([fitness(x, target) for x in pop])\n",
    "        return summed / (len(pop) * 1.0)\n",
    "\n",
    "    def evolve(self,pop, target, retain=0.2, random_select=0.05, mutate=0.01):\n",
    "        graded = [ (fitness(x, target), x) for x in pop]\n",
    "        graded = [ x[1] for x in sorted(graded)]\n",
    "        retain_length = int(len(graded)*retain)\n",
    "        parents = graded[:retain_length]\n",
    "        # randomly add other individuals to\n",
    "        # promote genetic diversity\n",
    "        for individual in graded[retain_length:]:\n",
    "            if random_select > random():\n",
    "                parents.append(individual)\n",
    "        # mutate some individuals\n",
    "        for individual in parents:\n",
    "            if mutate > random():\n",
    "                pos_to_mutate = randint(0, len(individual)-1)\n",
    "                # this mutation is not ideal, because it\n",
    "                # restricts the range of possible values,\n",
    "                # but the function is unaware of the min/max\n",
    "                # values used to create the individuals,\n",
    "                individual[pos_to_mutate] = randint(\n",
    "                    min(individual), max(individual))\n",
    "        # crossover parents to create children\n",
    "        parents_length = len(parents)\n",
    "        desired_length = len(pop) - parents_length\n",
    "        children = []\n",
    "        while len(children) < desired_length:\n",
    "            male = randint(0, parents_length-1)\n",
    "            female = randint(0, parents_length-1)\n",
    "            if male != female:\n",
    "                male = parents[male]\n",
    "                female = parents[female]\n",
    "                half = int(len(male) / 2)\n",
    "                child = male[:half] + female[half:]\n",
    "                children.append(child)\n",
    "        parents.extend(children)\n",
    "        return parents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x=evolution(x_train,x_test,100,100,360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'display_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3d2828f0734d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-893eea5f2470>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(self, individual, target)\u001b[0m\n\u001b[1;32m     47\u001b[0m                                           \u001b[0mencode_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                           optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-3527cdfad982>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, X_test, batch_size, max_epochs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Display logs per epoch step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%04d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0;34m\"cost=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{:.9f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_step' is not defined"
     ]
    }
   ],
   "source": [
    "x.population[randint(1,99)].len\n",
    "x.fitness(x.population[0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.05\n",
      "51.6\n",
      "33.75\n",
      "31.45\n",
      "19.49\n",
      "14.81\n",
      "8.27\n",
      "4.79\n",
      "0.38\n",
      "0.33\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.08\n",
      "0.44\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.72\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "1.8\n",
      "0.9\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.76\n",
      "0.07\n",
      "1.04\n",
      "0.0\n",
      "0.0\n",
      "3.6\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.14\n",
      "0.0\n",
      "1.84\n",
      "0.0\n",
      "0.02\n",
      "1.71\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.87\n",
      "1.45\n",
      "2.49\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.64\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.9\n",
      "0.0\n",
      "1.33\n",
      "0.76\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.25\n",
      "2.25\n",
      "1.35\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.64\n",
      "0.0\n",
      "0.04\n",
      "0.44\n",
      "1.17\n",
      "0.0\n",
      "2.36\n",
      "0.0\n",
      "0.0\n",
      "0.32\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.24\n",
      "0.32\n",
      "0.0\n",
      "0.0\n",
      "0.04\n",
      "0.1\n",
      "0.76\n",
      "0.0\n",
      "0.78\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.95\n",
      "1.97\n",
      "0.0\n",
      "1.95\n",
      "1.95\n",
      "1.5\n",
      "2.4\n",
      "2.4\n",
      "0.0\n",
      "0.0\n",
      "1.36\n",
      "0.0\n",
      "0.0\n",
      "0.3\n",
      "0.0\n",
      "1.95\n",
      "1.17\n",
      "0.0\n",
      "0.0\n",
      "0.43\n",
      "0.0\n",
      "2.52\n",
      "1.68\n",
      "2.6\n",
      "1.3\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.06\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.22\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.25\n",
      "0.45\n",
      "0.4\n",
      "0.0\n",
      "1.48\n",
      "0.0\n",
      "0.4\n",
      "0.0\n",
      "1.06\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.6\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.35\n",
      "0.0\n",
      "0.0\n",
      "0.6\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.77\n",
      "2.95\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.45\n",
      "0.0\n",
      "0.0\n",
      "1.17\n",
      "0.13\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.29\n",
      "0.0\n",
      "0.0\n",
      "0.69\n",
      "0.0\n",
      "2.1\n",
      "1.68\n",
      "2.94\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.84\n",
      "0.0\n",
      "0.04\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.9\n",
      "1.74\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.67\n",
      "2.35\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.02\n",
      "0.28\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.12\n",
      "0.0\n",
      "1.26\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.04\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.28\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.28\n",
      "0.06\n",
      "0.5\n",
      "0.0\n",
      "0.0\n",
      "1.2\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.36\n",
      "0.0\n",
      "0.48\n",
      "0.36\n",
      "0.84\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.48\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.34\n",
      "0.0\n",
      "0.0\n",
      "0.48\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.78\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.96\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.36\n",
      "0.0\n",
      "0.0\n",
      "1.43\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.08\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.36\n",
      "1.08\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.84\n",
      "0.96\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.3\n",
      "0.25\n",
      "0.0\n",
      "0.3\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.44\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.57\n",
      "0.0\n",
      "0.0\n",
      "0.09\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.72\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.08\n",
      "0.69\n",
      "0.6\n",
      "0.0\n",
      "2.15\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.04\n",
      "0.0\n",
      "0.0\n",
      "0.21\n",
      "0.0\n",
      "1.04\n",
      "1.82\n",
      "0.0\n",
      "1.14\n",
      "0.38\n",
      "0.0\n",
      "1.17\n",
      "1.04\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.55\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.56\n",
      "0.0\n",
      "0.64\n",
      "0.63\n",
      "0.0\n",
      "0.55\n",
      "0.22\n",
      "0.0\n",
      "0.0\n",
      "0.05\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.3\n",
      "1.44\n",
      "0.54\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4\n",
      "0.0\n",
      "0.0\n",
      "0.6\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.22\n",
      "0.0\n",
      "0.0\n",
      "1.41\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.52\n",
      "0.54\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.22\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.02\n",
      "2.25\n",
      "2.21\n",
      "0.08\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.3\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.52\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.8\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4\n",
      "0.3\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.1\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.36\n",
      "0.0\n",
      "0.92\n",
      "1.61\n",
      "1.38\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.38\n",
      "0.92\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.04\n",
      "2.28\n",
      "0.0\n",
      "0.0\n",
      "0.48\n",
      "0.52\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "4.7\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.8\n",
      "2.5\n",
      "0.6\n",
      "0.7\n",
      "0.6\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.08\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.02\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.12\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.68\n",
      "2.05\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.9\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.08\n",
      "0.0\n",
      "0.58\n",
      "0.0\n",
      "2.17\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "4.08\n",
      "2.52\n",
      "0.0\n",
      "1.32\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.72\n",
      "0.0\n",
      "0.0\n",
      "1.2\n",
      "1.2\n",
      "0.29\n",
      "0.0\n",
      "0.35\n",
      "0.55\n",
      "0.0\n",
      "4.0\n",
      "5.16\n",
      "3.12\n",
      "0.0\n",
      "0.96\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.89\n",
      "0.0\n",
      "0.16\n",
      "0.18\n",
      "0.46\n",
      "0.69\n",
      "2.35\n",
      "0.0\n",
      "1.74\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.32\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.24\n",
      "3.24\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.1\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "5.8\n",
      "1.32\n",
      "0.8\n",
      "1.26\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.22\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.44\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.68\n",
      "4.69\n",
      "2.01\n",
      "0.12\n",
      "0.55\n",
      "0.21\n",
      "0.27\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.98\n",
      "0.6\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.16\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.56\n",
      "0.42\n",
      "4.41\n",
      "5.04\n",
      "3.15\n",
      "0.0\n",
      "0.81\n",
      "0.0\n",
      "0.0\n",
      "0.16\n",
      "0.2\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.16\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.92\n",
      "0.0\n",
      "0.98\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.32\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.68\n",
      "0.84\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.24\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.06\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.34\n",
      "1.48\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.9\n",
      "0.0\n",
      "0.0\n",
      "0.87\n",
      "1.74\n",
      "0.36\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.24\n",
      "2.24\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.16\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.92\n",
      "1.12\n",
      "0.0\n",
      "0.0\n",
      "2.88\n",
      "0.16\n",
      "0.0\n",
      "0.3\n",
      "0.0\n",
      "0.0\n",
      "0.09\n",
      "0.12\n",
      "1.64\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "4.18\n",
      "2.79\n",
      "0.91\n",
      "0.0\n",
      "0.5\n",
      "2.01\n",
      "0.0\n",
      "1.22\n",
      "0.0\n",
      "0.0\n",
      "1.14\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.08\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.46\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target = 246\n",
    "p_count = 100\n",
    "i_length = 6\n",
    "i_min = 0\n",
    "i_max = 100\n",
    "p = population(p_count, i_length, i_min, i_max)\n",
    "fitness_history = [grade(p, target),]\n",
    "for i in range(1000):\n",
    "    p = evolve(p, target)\n",
    "    fitness_history.append(grade(p, target))\n",
    "    #print(p)\n",
    "for datum in fitness_history:\n",
    "   print(datum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
